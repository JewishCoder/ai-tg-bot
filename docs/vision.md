# Техническое видение проекта: LLM-ассистент Telegram-бот

## Введение

Данный документ описывает техническое видение проекта LLM-ассистента в виде Telegram-бота. Документ служит отправной точкой и техническим проектом для разработки MVP решения.

**Цель**: Создать максимально простое решение для проверки идеи интеллектуального Telegram-бота на основе LLM.

**Принципы**: KISS (Keep It Simple, Stupid) и ООП с принципом "один класс - один файл". Никакого оверинжиниринга.

---

## 1. Технологии

### Основной стек

- **Python 3.11+** - основной язык разработки
- **uv** - управление зависимостями и виртуальным окружением
- **aiogram 3.x** - асинхронный фреймворк для Telegram Bot API (polling)
- **openai** - клиент для работы с OpenRouter API
- **make** - автоматизация команд (run, install, clean)

### Вспомогательные библиотеки

- **python-dotenv** - загрузка конфигурации из `.env`
- **pydantic** - валидация конфигурации

### Хранение данных

- **In-memory** хранилище для активных диалогов
- **Файловая система** для персистентности (JSON файлы)
- **Ограничение истории**: последние N сообщений на пользователя

### Окружение

- **Polling** для получения обновлений от Telegram
- **Docker** для разработки (опционально, приложение работает и без Docker)
- Без внешней БД

### Обоснование выбора

- Все технологии проверенные и простые в использовании
- Минимальные требования к инфраструктуре
- Быстрый старт разработки и тестирования идеи

---

## 2. Принципы разработки

### 1. KISS (Keep It Simple, Stupid)
- Простые решения вместо сложных
- Избегаем преждевременной оптимизации
- Код должен быть понятен с первого взгляда

### 2. ООП с четкой структурой
- Один класс = один файл
- Один класс = одна ответственность (SRP)
- Никаких "божественных объектов"

### 3. Читаемость кода
- Говорящие имена переменных и функций
- Короткие функции (до 20-30 строк)
- Минимум вложенности

### 4. Документирование
- Docstring для всех публичных методов и функций
- Описание параметров, возвращаемых значений и исключений
- Документирование контрактов (интерфейсов/протоколов)

### 5. Логирование
- Логируем все ключевые события приложения
- Разные уровни логов (DEBUG, INFO, WARNING, ERROR)
- Логи помогают отслеживать работу и находить проблемы

### 6. Асинхронность
- Используем async/await где необходимо
- Не блокируем event loop

### 7. Минимализм
- Только необходимые зависимости
- Только необходимый функционал для MVP
- Без "а вдруг пригодится"

### 8. Типизация
- Type hints для всех функций и методов
- Помогает избежать ошибок и улучшает читаемость

---

## 3. Структура проекта

```
ai-tg-bot/
├── src/
│   ├── __init__.py
│   ├── bot.py              # Основной класс бота (Bot)
│   ├── config.py           # Конфигурация (Config)
│   ├── llm_client.py       # Клиент для работы с LLM (LLMClient)
│   ├── message_handler.py  # Обработчик сообщений (MessageHandler)
│   ├── storage.py          # Хранилище диалогов (Storage)
│   └── main.py             # Точка входа
├── data/                   # JSON файлы с историей диалогов
│   └── .gitkeep
├── logs/                   # Логи приложения
│   └── .gitkeep
├── docs/                   # Документация
│   ├── idea.md
│   └── vision.md
├── tests/                  # Тесты (для будущего)
│   └── __init__.py
├── .env.example            # Пример конфигурации
├── .env                    # Конфигурация (не в git)
├── .gitignore
├── .dockerignore           # Исключения для Docker
├── Dockerfile.dev          # Docker образ для разработки
├── docker-compose.yml      # Docker Compose конфигурация
├── Makefile                # Команды для управления проектом
├── pyproject.toml          # uv конфигурация и зависимости
└── README.md
```

### Описание компонентов

**Исходный код:**
- **main.py** - точка входа, запуск приложения, парсинг CLI аргументов
- **bot.py** - инициализация и запуск Telegram бота
- **config.py** - загрузка и валидация настроек из `.env`
- **llm_client.py** - взаимодействие с OpenRouter API
- **message_handler.py** - логика обработки команд и сообщений пользователей
- **storage.py** - сохранение/загрузка истории диалогов в JSON

**Данные и логи:**
- **data/** - директория для хранения JSON файлов с историей
- **logs/** - директория для файлов логов

**DevOps и конфигурация:**
- **Dockerfile.dev** - Docker образ для разработки (Alpine-based)
- **docker-compose.yml** - оркестрация Docker контейнера
- **.dockerignore** - исключения при сборке образа
- **Makefile** - команды для запуска и управления проектом
- **.env** / **.env.example** - переменные окружения

### Принципы организации

- Один класс = один файл
- Плоская структура без глубокой вложенности
- Понятные имена файлов, соответствующие назначению

---

## 4. Архитектура проекта

### Компоненты системы

#### 1. Bot
- Инициализирует aiogram dispatcher и bot
- Регистрирует handlers
- Запускает polling
- Координирует работу всех компонентов

#### 2. MessageHandler
- Получает команды и сообщения от пользователя
- Обрабатывает команды: `/start`, `/role`, `/reset`, `/status`, `/help`
- Для обычных сообщений:
  - Показывает typing action (индикатор "печатает...")
  - Загружает историю диалога через Storage
  - Отправляет запрос в LLMClient с историей
  - Сохраняет обновленную историю
  - Отправляет ответ пользователю (разбивает на части если >4096 символов)

#### 3. LLMClient
- Формирует запрос к OpenRouter API
- Добавляет системный промпт к истории
- Отправляет запрос и получает ответ модели
- Возвращает текст ответа

#### 4. Storage
- Загрузка истории диалога из JSON файла
- Сохранение истории диалога в JSON файл
- Ограничение количества сообщений в истории
- Управление файлами по user_id

#### 5. Config
- Загрузка переменных окружения из .env
- Валидация настроек через Pydantic
- Предоставление настроек другим компонентам

### Поток данных

```
Пользователь Telegram
        ↓
    Bot (aiogram)
        ↓
  MessageHandler ←→ Storage ←→ JSON файлы
        ↓
   LLMClient
        ↓
  OpenRouter API
        ↓
    Ответ LLM
        ↓
  MessageHandler
        ↓
    Bot (aiogram)
        ↓
Пользователь Telegram
```

### Принципы взаимодействия

- Слабая связанность компонентов
- Каждый компонент решает одну задачу
- Простые интерфейсы между компонентами
- Асинхронное взаимодействие где необходимо

---

## 5. Модель данных

### 1. Сообщение (Message)

```python
{
    "role": "user" | "assistant" | "system",
    "content": "текст сообщения",
    "timestamp": "2025-10-10T12:00:00"
}
```

### 2. История диалога (Dialog)

```python
{
    "user_id": 123456789,
    "messages": [
        {
            "role": "system", 
            "content": "системный промпт",
            "timestamp": "2025-10-10T12:00:00"
        },
        {
            "role": "user", 
            "content": "привет",
            "timestamp": "2025-10-10T12:00:05"
        },
        {
            "role": "assistant", 
            "content": "здравствуйте",
            "timestamp": "2025-10-10T12:00:07"
        }
    ],
    "updated_at": "2025-10-10T12:00:07"
}
```

### 3. Конфигурация (Config)

```python
{
    "telegram_token": "...",
    "openrouter_api_key": "...",
    "openrouter_base_url": "https://openrouter.ai/api/v1",
    "openrouter_model": "anthropic/claude-3.5-sonnet",
    "system_prompt": "Ты полезный ассистент...",
    "max_history_messages": 50,
    "llm_temperature": 0.7,
    "llm_max_tokens": 1000,
    "retry_attempts": 3,
    "retry_delay": 1.0,
    "log_level": "INFO",
    "data_dir": "data",
    "logs_dir": "logs"
}
```

### Хранение на диске

- Один JSON файл на пользователя: `data/{user_id}.json`
- Формат файла соответствует структуре Dialog
- Timestamp в формате ISO 8601
- Автоматическое обновление `updated_at` при каждом сохранении

### Принципы работы с данными

- Простая JSON сериализация/десериализация
- Валидация через Pydantic модели
- Ограничение истории для экономии памяти и токенов LLM

---

## 6. Работа с LLM

### Подключение к OpenRouter

- Используем библиотеку `openai`
- OpenRouter совместим с OpenAI API
- Базовый URL из конфигурации: `openrouter_base_url`
- API ключ из конфигурации: `openrouter_api_key`
- Модель из конфигурации: `openrouter_model`

### Формирование запроса

1. Загружаем историю диалога пользователя из Storage
2. Добавляем системный промпт первым сообщением (если его нет в истории)
3. Ограничиваем количество сообщений (`max_history_messages`)
4. Формируем запрос в формате OpenAI Chat Completions API

### Параметры запроса

- `model` - из конфигурации (`openrouter_model`)
- `messages` - история диалога с системным промптом
- `temperature` - 0.7 (из конфигурации `llm_temperature`)
- `max_tokens` - 1000 (из конфигурации `llm_max_tokens`)

### Обработка ответа

- Извлекаем текст ответа из `choices[0].message.content`
- **Логируем использование токенов**: 
  - `prompt_tokens` - токены в запросе
  - `completion_tokens` - токены в ответе
  - `total_tokens` - общее количество
- Возвращаем текст ответа для отправки пользователю

### Обработка ошибок и retry

- **Retry механизм** для временных сбоев:
  - Количество попыток: `retry_attempts` (по умолчанию 3)
  - Задержка между попытками: `retry_delay` с экспоненциальным ростом
- Обрабатываем типы ошибок:
  - Rate limit errors
  - Timeout errors
  - Connection errors
  - API errors
- Логируем все ошибки с полным контекстом
- При провале всех попыток возвращаем понятное сообщение пользователю

### Логирование LLM операций

- Каждый запрос: модель, количество сообщений в истории, user_id
- Каждый ответ: использованные токены, время выполнения запроса
- Все ошибки: тип ошибки, детали, количество попыток
- Уровни логирования:
  - INFO - успешные запросы и ответы
  - WARNING - retry попытки
  - ERROR - финальные ошибки после всех retry

---

## 7. Сценарии работы

### 1. Начало работы - `/start`

- Пользователь отправляет команду `/start`
- Бот приветствует и объясняет свои возможности
- Создается новый диалог с системным промптом по умолчанию
- Показывает список доступных команд

### 2. Основной сценарий - диалог

- Пользователь отправляет текстовое сообщение
- **Бот показывает индикатор "печатает..."** (typing action)
- Загружает историю диалога из Storage
- Отправляет запрос в LLM с историей
- Получает ответ от LLM
- Сохраняет обновленную историю (user message + assistant message)
- Отправляет ответ пользователю
- При длинных ответах (>4096 символов) - разбивает на несколько сообщений

### 3. Установка роли - `/role`

**Установка кастомной роли:**
- Команда: `/role <текст системного промпта>`
- Пример: `/role Ты опытный Python разработчик. Помогаешь с кодом и архитектурой.`
- Без ограничений на длину промпта

**Возврат к роли по умолчанию:**
- Команда: `/role default`
- Устанавливает системный промпт из конфигурации

**Поведение:**
- Очищает текущую историю диалога (начинает новый диалог с новой ролью)
- Сохраняет новый системный промпт в файле пользователя
- Подтверждает установку роли с первыми 100 символами промпта

### 4. Сброс истории - `/reset`

- Очищает историю сообщений
- Сохраняет текущий системный промпт (не сбрасывает роль)
- Подтверждает сброс с информацией о текущей роли

### 5. Статус - `/status`

Показывает информацию:
- Количество сообщений в текущей истории
- Текущий системный промпт (первые 100 символов)
- Дата последнего обновления диалога
- Используемая модель LLM

### 6. Справка - `/help`

Список всех команд:
- `/start` - начать работу с ботом
- `/role <промпт>` - установить роль ассистента
- `/role default` - вернуться к роли по умолчанию
- `/reset` - очистить историю диалога
- `/status` - показать статус и статистику
- `/help` - показать эту справку

### Обработка ошибок

- LLM недоступен → сообщаем и предлагаем повторить позже
- Некорректная команда → подсказываем `/help`
- `/role` без текста → показываем пример использования

---

## 8. Конфигурирование

### Способ конфигурирования

- **Файл `.env`** - все настройки в одном месте (по умолчанию в корне проекта)
- **Переменные окружения** - приоритет над .env (для продакшена)
- **Аргумент командной строки** - `--env-file` для указания пути к .env (через встроенный `argparse`)
- **Pydantic** - валидация и типизация конфигурации
- **`.env.example`** - шаблон с описанием всех параметров

### Структура конфигурации (.env)

```bash
# Telegram Bot
TELEGRAM_TOKEN=your_bot_token_here

# OpenRouter LLM
OPENROUTER_API_KEY=your_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_MODEL=anthropic/claude-3.5-sonnet

# System Prompt
SYSTEM_PROMPT=Ты полезный ассистент. Отвечай на вопросы пользователей четко и по делу.

# LLM Parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1000
MAX_HISTORY_MESSAGES=50

# Retry Configuration
RETRY_ATTEMPTS=3
RETRY_DELAY=1.0

# Directories
DATA_DIR=data
LOGS_DIR=logs

# Logging
LOG_LEVEL=INFO
```

### Запуск с кастомным .env

```bash
# Запуск с .env из другого места
python -m src.main --env-file /path/to/custom.env

# Запуск с .env по умолчанию
python -m src.main
```

**Примечание:** Парсинг аргументов командной строки реализован через встроенный модуль `argparse` (без дополнительных зависимостей).

### Принципы конфигурирования

1. **Все секреты в .env** - токены и ключи не в коде, не в git
2. **Значения по умолчанию** - разумные дефолты для необязательных параметров
3. **Валидация при старте** - приложение не запустится с невалидной конфигурацией
4. **Понятные имена** - каждый параметр самоочевиден
5. **Один источник истины** - Config класс предоставляет настройки всем компонентам
6. **Гибкость** - можно использовать разные .env для разных окружений

### Обработка ошибок конфигурации

- Отсутствует обязательный параметр → понятное сообщение какой именно параметр нужен
- Невалидное значение → сообщение с ожидаемым форматом/типом
- Файл .env не найден → попытка использовать переменные окружения или ошибка
- Невалидный путь к .env → понятное сообщение об ошибке

### Приоритет загрузки настроек

1. Аргументы командной строки (путь к .env)
2. Переменные окружения
3. Файл .env
4. Значения по умолчанию (если применимо)

---

## 9. Логирование

### Инструменты

- **Встроенный модуль `logging`** Python - без дополнительных зависимостей
- **RotatingFileHandler** - ротация логов по размеру (встроенный)
- **Форматированный вывод** - читаемые логи с временем и контекстом

### Уровни логирования

- **DEBUG** - детальная информация для отладки (все вызовы API, детали Storage)
- **INFO** - ключевые события (старт бота, обработка команд, успешные LLM запросы)
- **WARNING** - предупреждения (retry попытки, длинные сообщения)
- **ERROR** - ошибки (провал LLM запроса, ошибки Storage, исключения)

### Что логируем

**Запуск приложения:**
- Версия Python, загруженная конфигурация (без секретов)
- Инициализация компонентов (Bot, Storage, LLMClient)
- Успешный запуск polling

**Telegram бот:**
- Входящие команды (user_id, команда)
- Входящие сообщения (user_id, длина текста)
- Отправленные ответы (user_id, длина ответа)

**LLM операции:**
- Запросы (user_id, модель, количество сообщений в истории)
- Ответы (использованные токены, время выполнения)
- Ошибки и retry попытки

**Storage:**
- Чтение/запись файлов (user_id, путь к файлу)
- Создание новых диалогов
- Ошибки работы с файловой системой

**Ошибки и исключения:**
- Полный stack trace
- Контекст выполнения (что делали, какие данные обрабатывали)

### Формат лога

```
2025-10-10 12:00:00,123 | INFO     | bot.py:45              | Bot started successfully
2025-10-10 12:00:05,456 | INFO     | message_handler.py:78  | User 123456: received message (15 chars)
2025-10-10 12:00:05,500 | INFO     | llm_client.py:120      | LLM request: model=claude-3.5-sonnet, messages=5
2025-10-10 12:00:07,890 | INFO     | llm_client.py:145      | LLM response: tokens=150/50/200, time=2.3s
2025-10-10 12:00:10,123 | WARNING  | llm_client.py:130      | LLM request failed, retrying (attempt 1/3)
2025-10-10 12:00:15,789 | ERROR    | storage.py:89          | Failed to save dialog: [Errno 13] Permission denied
```

### Хранение логов

**Консоль (stdout):**
- Используется при разработке
- Тот же формат, без цветов
- Уровень из конфигурации

**Файл:**
- Путь: `logs/bot.log`
- Ротация по размеру: максимум 10MB на файл
- Хранение: последние 5 файлов
- Формат: текст, одна строка = один лог-запись
- Легко парсится стандартными инструментами

### Конфигурация логирования

- Уровень: из переменной `LOG_LEVEL` в .env
- Директория: из переменной `LOGS_DIR` в .env
- Настройка: в `main.py` при старте приложения
- Автоматическое создание директории `logs/` если не существует

### Принципы логирования

1. **Не логируем секреты** - токены, API ключи маскируются
2. **Контекст всегда** - каждый лог содержит достаточно информации
3. **Читаемость** - логи должны быть понятны человеку
4. **Производительность** - минимальное влияние на работу приложения
5. **Простота** - используем только встроенные средства Python

---

## 10. DevOps

### Цель

Унифицированная среда разработки в Docker контейнере для изоляции и воспроизводимости окружения.

### Docker для разработки

**Базовый образ:**
- `python:3.11-alpine` - минимальный оптимизированный образ (~50MB)
- Установка только необходимых системных зависимостей

**Особенности контейнера:**
- Volume mounting для live-reload кода (изменения применяются сразу)
- Проброс директорий: `./src`, `./data`, `./logs`
- Поддержка переменных окружения из `.env`
- Кеширование зависимостей для ускорения пересборки

**Dockerfile.dev структура:**
- Multi-stage сборка для оптимизации
- Отдельный слой для установки зависимостей (кеширование)
- Рабочая директория `/app`
- Non-root пользователь для безопасности

**docker-compose.yml:**
- Один сервис для бота
- Автоматический restart при сбоях
- Volume mapping для кода и данных
- Переменные окружения из `.env`

### Оптимизация контейнера

**Alpine Linux:**
- Минимальный размер базового образа (~50MB вместо ~900MB Debian)
- Быстрая загрузка и сборка
- apk package manager

**Кеширование:**
- Зависимости устанавливаются отдельным слоем
- Пересборка только при изменении `pyproject.toml`
- Код копируется последним слоем

**Минимализм:**
- Только необходимые системные пакеты
- `.dockerignore` исключает ненужные файлы
- Отсутствие dev-инструментов в контейнере

### Makefile команды

```makefile
# Docker команды для разработки
docker-build      # Собрать образ для разработки
docker-up         # Запустить контейнер (с live reload)
docker-down       # Остановить контейнер
docker-logs       # Показать логи контейнера
docker-shell      # Войти в shell контейнера
docker-restart    # Перезапустить контейнер
docker-clean      # Удалить контейнер и volumes

# Локальные команды (без Docker)
install           # Установить зависимости через uv
run               # Запустить бота локально
clean             # Очистить временные файлы
```

### Структура файлов

```
ai-tg-bot/
├── Dockerfile.dev           # Образ для разработки (Alpine)
├── docker-compose.yml       # Оркестрация для разработки
└── .dockerignore            # Исключения для Docker
```

### Два режима работы

**1. С Docker (рекомендуется для разработки):**
```bash
make docker-up
# Контейнер запускается с volume mapping
# Изменения в коде применяются автоматически
```

**2. Без Docker (локальный запуск):**
```bash
make install
make run
# Обычный запуск Python приложения
```

### Преимущества Docker для разработки

- ✅ Изолированное окружение
- ✅ Одинаковая среда у всех разработчиков
- ✅ Не загрязняет систему зависимостями
- ✅ Легко переключаться между проектами
- ✅ Воспроизводимость багов

### Принципы

1. **Простота** - минимальная конфигурация Docker
2. **Производительность** - Alpine образ + кеширование слоев
3. **Гибкость** - приложение работает с Docker и без него
4. **Изоляция** - окружение разработки не влияет на систему

---

## Заключение

Данный документ описывает техническое видение MVP проекта LLM-ассистента в виде Telegram-бота. Все решения направлены на создание максимально простого и работающего решения для проверки идеи.

**Ключевые принципы:**
- KISS - простота во всем
- Минимум зависимостей
- Быстрый старт разработки
- Легкость поддержки и развития

Документ служит основой для последующей разработки и может дополняться по мере необходимости.

