"""–¢–µ—Å—Ç—ã –¥–ª—è –º–æ–¥—É–ª—è LLMClient."""

from unittest.mock import AsyncMock, MagicMock

import pytest
from openai import APIConnectionError, APIError, RateLimitError

from src.config import Config
from src.llm_client import LLMAPIError, LLMClient


class TestLLMClient:
    """–¢–µ—Å—Ç—ã –∫–ª–∞—Å—Å–∞ LLMClient."""

    @pytest.mark.asyncio
    async def test_init_creates_client(self, test_config: Config) -> None:
        """
        –¢–µ—Å—Ç: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLMClient —Å–æ–∑–¥–∞—ë—Ç AsyncOpenAI –∫–ª–∏–µ–Ω—Ç–∞.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        """
        llm_client = LLMClient(test_config)

        assert llm_client.config == test_config
        assert llm_client.client is not None

    @pytest.mark.asyncio
    async def test_generate_response_success(
        self,
        test_config: Config,
        mock_openai_client: AsyncMock,
        sample_messages: list[dict[str, str]],
    ) -> None:
        """
        –¢–µ—Å—Ç: —É—Å–ø–µ—à–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            mock_openai_client: Mock –∫–ª–∏–µ–Ω—Ç–∞ OpenAI
            sample_messages: –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        llm_client = LLMClient(test_config)
        llm_client.client = mock_openai_client
        user_id = 12345

        response = await llm_client.generate_response(sample_messages, user_id)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –æ—Ç–≤–µ—Ç
        assert response == "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM."

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ API –±—ã–ª –≤—ã–∑–≤–∞–Ω —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        mock_openai_client.chat.completions.create.assert_called_once()
        call_kwargs = mock_openai_client.chat.completions.create.call_args.kwargs

        assert call_kwargs["model"] == test_config.openrouter_model
        assert call_kwargs["temperature"] == test_config.llm_temperature
        assert call_kwargs["max_tokens"] == test_config.llm_max_tokens
        assert "messages" in call_kwargs

    @pytest.mark.asyncio
    async def test_generate_response_filters_timestamp(
        self,
        test_config: Config,
        mock_openai_client: AsyncMock,
        sample_messages: list[dict[str, str]],
    ) -> None:
        """
        –¢–µ—Å—Ç: timestamp —É–¥–∞–ª—è–µ—Ç—Å—è –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π –≤ API.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            mock_openai_client: Mock –∫–ª–∏–µ–Ω—Ç–∞ OpenAI
            sample_messages: –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        llm_client = LLMClient(test_config)
        llm_client.client = mock_openai_client
        user_id = 12345

        await llm_client.generate_response(sample_messages, user_id)

        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        call_kwargs = mock_openai_client.chat.completions.create.call_args.kwargs
        sent_messages = call_kwargs["messages"]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ timestamp –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
        for msg in sent_messages:
            assert "timestamp" not in msg
            assert "role" in msg
            assert "content" in msg

    @pytest.mark.asyncio
    async def test_retry_on_rate_limit_error(
        self, test_config: Config, sample_messages: list[dict[str, str]]
    ) -> None:
        """
        –¢–µ—Å—Ç: retry –º–µ—Ö–∞–Ω–∏–∑–º –ø—Ä–∏ RateLimitError.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            sample_messages: –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        llm_client = LLMClient(test_config)
        user_id = 12345

        # –°–æ–∑–¥–∞—ë–º mock –∫–æ—Ç–æ—Ä—ã–π —Å–Ω–∞—á–∞–ª–∞ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç RateLimitError, –ø–æ—Ç–æ–º —É—Å–ø–µ—Ö
        mock_client = AsyncMock()
        mock_choice = AsyncMock()
        mock_choice.message.content = "–£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ retry"

        mock_completion = AsyncMock()
        mock_completion.choices = [mock_choice]
        mock_completion.usage.prompt_tokens = 10
        mock_completion.usage.completion_tokens = 5
        mock_completion.usage.total_tokens = 15

        # –°–æ–∑–¥–∞—ë–º mock response –¥–ª—è RateLimitError
        mock_response = MagicMock()
        mock_response.request = MagicMock()

        # –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ - –æ—à–∏–±–∫–∞, –≤—Ç–æ—Ä–æ–π - —É—Å–ø–µ—Ö
        mock_client.chat.completions.create.side_effect = [
            RateLimitError("Rate limit exceeded", response=mock_response, body=None),
            mock_completion,
        ]

        llm_client.client = mock_client

        # –í—ã–∑—ã–≤–∞–µ–º –∏ –æ–∂–∏–¥–∞–µ–º —É—Å–ø–µ—à–Ω—ã–π retry
        response = await llm_client.generate_response(sample_messages, user_id)

        assert response == "–£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ retry"
        assert mock_client.chat.completions.create.call_count == 2

    @pytest.mark.asyncio
    async def test_retry_on_connection_error(
        self, test_config: Config, sample_messages: list[dict[str, str]]
    ) -> None:
        """
        –¢–µ—Å—Ç: retry –º–µ—Ö–∞–Ω–∏–∑–º –ø—Ä–∏ APIConnectionError.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            sample_messages: –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        llm_client = LLMClient(test_config)
        user_id = 12345

        # –°–æ–∑–¥–∞—ë–º mock
        mock_client = AsyncMock()
        mock_choice = AsyncMock()
        mock_choice.message.content = "–£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è"

        mock_completion = AsyncMock()
        mock_completion.choices = [mock_choice]
        mock_completion.usage.prompt_tokens = 10
        mock_completion.usage.completion_tokens = 5
        mock_completion.usage.total_tokens = 15

        # –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ - –æ—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è, –≤—Ç–æ—Ä–æ–π - —É—Å–ø–µ—Ö
        mock_request = AsyncMock()
        mock_client.chat.completions.create.side_effect = [
            APIConnectionError(request=mock_request),
            mock_completion,
        ]

        llm_client.client = mock_client

        response = await llm_client.generate_response(sample_messages, user_id)

        assert response == "–£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è"
        assert mock_client.chat.completions.create.call_count == 2

    @pytest.mark.asyncio
    async def test_max_retries_exceeded(
        self, test_config: Config, sample_messages: list[dict[str, str]]
    ) -> None:
        """
        –¢–µ—Å—Ç: –≤—ã–±—Ä–æ—Å –∏—Å–∫–ª—é—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ retry.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            sample_messages: –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        # –û—Ç–∫–ª—é—á–∞–µ–º fallback –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–æ–ª—å–∫–æ retry –º–µ—Ö–∞–Ω–∏–∑–º
        test_config.openrouter_fallback_model = None
        llm_client = LLMClient(test_config)
        user_id = 12345

        # –°–æ–∑–¥–∞—ë–º mock –∫–æ—Ç–æ—Ä—ã–π –≤—Å–µ–≥–¥–∞ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç –æ—à–∏–±–∫—É
        mock_client = AsyncMock()
        # –°–æ–∑–¥–∞—ë–º mock response –¥–ª—è RateLimitError
        mock_response = MagicMock()
        mock_response.request = MagicMock()

        mock_client.chat.completions.create.side_effect = RateLimitError(
            "Rate limit exceeded", response=mock_response, body=None
        )

        llm_client.client = mock_client

        # –û–∂–∏–¥–∞–µ–º LLMAPIError –ø–æ—Å–ª–µ –≤—Å–µ—Ö retry
        with pytest.raises(LLMAPIError) as exc_info:
            await llm_client.generate_response(sample_messages, user_id)

        assert "Rate limit exceeded" in str(exc_info.value)
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 3 –ø–æ–ø—ã—Ç–∫–∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ (–±–µ–∑ fallback)
        assert mock_client.chat.completions.create.call_count == test_config.retry_attempts

    @pytest.mark.asyncio
    async def test_api_error_handling(
        self, test_config: Config, sample_messages: list[dict[str, str]]
    ) -> None:
        """
        –¢–µ—Å—Ç: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—â–∏—Ö –æ—à–∏–±–æ–∫ API.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            sample_messages: –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        llm_client = LLMClient(test_config)
        user_id = 12345

        # –°–æ–∑–¥–∞—ë–º mock –∫–æ—Ç–æ—Ä—ã–π –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç APIError
        mock_client = AsyncMock()
        mock_client.chat.completions.create.side_effect = APIError(
            "API Error", request=AsyncMock(), body=None
        )

        llm_client.client = mock_client

        # –û–∂–∏–¥–∞–µ–º LLMAPIError
        with pytest.raises(LLMAPIError):
            await llm_client.generate_response(sample_messages, user_id)

    @pytest.mark.asyncio
    async def test_empty_response_handling(
        self, test_config: Config, sample_messages: list[dict[str, str]]
    ) -> None:
        """
        –¢–µ—Å—Ç: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Å—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –æ—Ç API.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            sample_messages: –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        llm_client = LLMClient(test_config)
        user_id = 12345

        # –°–æ–∑–¥–∞—ë–º mock —Å –ø—É—Å—Ç—ã–º –æ—Ç–≤–µ—Ç–æ–º
        mock_client = AsyncMock()
        mock_choice = AsyncMock()
        mock_choice.message.content = None

        mock_completion = AsyncMock()
        mock_completion.choices = [mock_choice]
        mock_completion.usage.prompt_tokens = 10
        mock_completion.usage.completion_tokens = 0
        mock_completion.usage.total_tokens = 10

        mock_client.chat.completions.create.return_value = mock_completion
        llm_client.client = mock_client

        # LLMClient –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –µ—Å–ª–∏ content None (–Ω–µ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ)
        response = await llm_client.generate_response(sample_messages, user_id)
        assert response == ""

    @pytest.mark.asyncio
    async def test_no_choices_in_response(
        self, test_config: Config, sample_messages: list[dict[str, str]]
    ) -> None:
        """
        –¢–µ—Å—Ç: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ —Å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º choices.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            sample_messages: –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        llm_client = LLMClient(test_config)
        user_id = 12345

        # –°–æ–∑–¥–∞—ë–º mock —Å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º choices
        mock_client = AsyncMock()
        mock_completion = AsyncMock()
        mock_completion.choices = []

        mock_client.chat.completions.create.return_value = mock_completion
        llm_client.client = mock_client

        # –û–∂–∏–¥–∞–µ–º LLMAPIError —Å –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –æ—à–∏–±–∫–æ–π
        with pytest.raises(LLMAPIError) as exc_info:
            await llm_client.generate_response(sample_messages, user_id)

        # –û—à–∏–±–∫–∞ –±—É–¥–µ—Ç —è–≤–Ω–æ –≥–æ–≤–æ—Ä–∏—Ç—å –æ –ø—Ä–æ–±–ª–µ–º–µ
        assert "no choices in response" in str(exc_info.value)

    @pytest.mark.asyncio
    async def test_none_choices_in_response(
        self, test_config: Config, sample_messages: list[dict[str, str]]
    ) -> None:
        """
        –¢–µ—Å—Ç: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –≥–¥–µ choices —ç—Ç–æ None.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            sample_messages: –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        llm_client = LLMClient(test_config)
        user_id = 12345

        # –°–æ–∑–¥–∞—ë–º mock –≥–¥–µ choices = None
        mock_client = AsyncMock()
        mock_completion = AsyncMock()
        mock_completion.choices = None

        mock_client.chat.completions.create.return_value = mock_completion
        llm_client.client = mock_client

        # –û–∂–∏–¥–∞–µ–º LLMAPIError —Å –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –æ—à–∏–±–∫–æ–π
        with pytest.raises(LLMAPIError) as exc_info:
            await llm_client.generate_response(sample_messages, user_id)

        # –û—à–∏–±–∫–∞ –±—É–¥–µ—Ç —è–≤–Ω–æ –≥–æ–≤–æ—Ä–∏—Ç—å –æ –ø—Ä–æ–±–ª–µ–º–µ
        assert "no choices in response" in str(exc_info.value)

    @pytest.mark.asyncio
    async def test_none_message_in_choice(
        self, test_config: Config, sample_messages: list[dict[str, str]]
    ) -> None:
        """
        –¢–µ—Å—Ç: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –≥–¥–µ message –≤ choice —ç—Ç–æ None.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            sample_messages: –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        llm_client = LLMClient(test_config)
        user_id = 12345

        # –°–æ–∑–¥–∞—ë–º mock –≥–¥–µ choices[0].message = None
        mock_client = AsyncMock()
        mock_choice = AsyncMock()
        mock_choice.message = None

        mock_completion = AsyncMock()
        mock_completion.choices = [mock_choice]

        mock_client.chat.completions.create.return_value = mock_completion
        llm_client.client = mock_client

        # –û–∂–∏–¥–∞–µ–º LLMAPIError —Å –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –æ—à–∏–±–∫–æ–π
        with pytest.raises(LLMAPIError) as exc_info:
            await llm_client.generate_response(sample_messages, user_id)

        # –û—à–∏–±–∫–∞ –±—É–¥–µ—Ç —è–≤–Ω–æ –≥–æ–≤–æ—Ä–∏—Ç—å –æ –ø—Ä–æ–±–ª–µ–º–µ
        assert "no message in choice" in str(exc_info.value)

    @pytest.mark.asyncio
    async def test_usage_logging(
        self,
        test_config: Config,
        mock_openai_client: AsyncMock,
        sample_messages: list[dict[str, str]],
    ) -> None:
        """
        –¢–µ—Å—Ç: –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            mock_openai_client: Mock –∫–ª–∏–µ–Ω—Ç–∞ OpenAI
            sample_messages: –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        llm_client = LLMClient(test_config)
        llm_client.client = mock_openai_client
        user_id = 12345

        # –í—ã–∑—ã–≤–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
        await llm_client.generate_response(sample_messages, user_id)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–µ—Ç–æ–¥ –±—ã–ª –≤—ã–∑–≤–∞–Ω –∏ –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã–µ —Å usage
        # (–ø—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ caplog —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ñ–∏–∫—Å—Ç—É—Ä—ã,
        # –∑–¥–µ—Å—å –ø—Ä–æ—Å—Ç–æ —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –∫–æ–¥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫)
        assert mock_openai_client.chat.completions.create.called


class TestLLMClientFallback:
    """–¢–µ—Å—Ç—ã fallback –º–µ—Ö–∞–Ω–∏–∑–º–∞ LLMClient."""

    @pytest.mark.asyncio
    async def test_should_try_fallback_on_rate_limit(self, test_config: Config) -> None:
        """
        –¢–µ—Å—Ç: RateLimitError —Ç—Ä–∏–≥–≥–µ—Ä–∏—Ç fallback.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        """
        # Arrange: –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ —Å fallback –º–æ–¥–µ–ª—å—é
        test_config.openrouter_fallback_model = "meta-llama/llama-3.1-8b-instruct:free"
        llm_client = LLMClient(test_config)

        # –°–æ–∑–¥–∞—ë–º RateLimitError
        mock_response = MagicMock()
        mock_response.request = MagicMock()
        error = RateLimitError("Rate limit exceeded", response=mock_response, body=None)

        # Act: –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–ª–∂–µ–Ω –ª–∏ —Å—Ä–∞–±–æ—Ç–∞—Ç—å fallback
        should_fallback = llm_client._should_try_fallback(error)

        # Assert: RateLimitError –¥–æ–ª–∂–µ–Ω —Ç—Ä–∏–≥–≥–µ—Ä–∏—Ç—å fallback
        assert should_fallback is True

    @pytest.mark.asyncio
    async def test_should_try_fallback_on_api_error(self, test_config: Config) -> None:
        """
        –¢–µ—Å—Ç: APIError —Ç—Ä–∏–≥–≥–µ—Ä–∏—Ç fallback.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        """
        # Arrange: –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ —Å fallback –º–æ–¥–µ–ª—å—é
        test_config.openrouter_fallback_model = "meta-llama/llama-3.1-8b-instruct:free"
        llm_client = LLMClient(test_config)

        # –°–æ–∑–¥–∞—ë–º APIError
        error = APIError("Server error", request=AsyncMock(), body=None)

        # Act: –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–ª–∂–µ–Ω –ª–∏ —Å—Ä–∞–±–æ—Ç–∞—Ç—å fallback
        should_fallback = llm_client._should_try_fallback(error)

        # Assert: APIError –¥–æ–ª–∂–µ–Ω —Ç—Ä–∏–≥–≥–µ—Ä–∏—Ç—å fallback
        assert should_fallback is True

    @pytest.mark.asyncio
    async def test_should_not_fallback_when_no_fallback_model(self, test_config: Config) -> None:
        """
        –¢–µ—Å—Ç: fallback –Ω–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –µ—Å–ª–∏ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        """
        # Arrange: –ù–ï –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º fallback –º–æ–¥–µ–ª—å
        test_config.openrouter_fallback_model = None
        llm_client = LLMClient(test_config)

        # –°–æ–∑–¥–∞—ë–º RateLimitError
        mock_response = MagicMock()
        mock_response.request = MagicMock()
        error = RateLimitError("Rate limit exceeded", response=mock_response, body=None)

        # Act: –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–ª–∂–µ–Ω –ª–∏ —Å—Ä–∞–±–æ—Ç–∞—Ç—å fallback
        should_fallback = llm_client._should_try_fallback(error)

        # Assert: –±–µ–∑ fallback –º–æ–¥–µ–ª–∏ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å fallback
        assert should_fallback is False

    @pytest.mark.asyncio
    async def test_should_not_fallback_on_timeout(self, test_config: Config) -> None:
        """
        –¢–µ—Å—Ç: Timeout –æ—à–∏–±–∫–∏ –ù–ï —Ç—Ä–∏–≥–≥–µ—Ä—è—Ç fallback.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        """
        # Arrange: –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ —Å fallback –º–æ–¥–µ–ª—å—é
        test_config.openrouter_fallback_model = "meta-llama/llama-3.1-8b-instruct:free"
        llm_client = LLMClient(test_config)

        # –°–æ–∑–¥–∞—ë–º APITimeoutError
        from openai import APITimeoutError

        error = APITimeoutError(request=AsyncMock())

        # Act: –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–ª–∂–µ–Ω –ª–∏ —Å—Ä–∞–±–æ—Ç–∞—Ç—å fallback
        should_fallback = llm_client._should_try_fallback(error)

        # Assert: Timeout –ù–ï –¥–æ–ª–∂–µ–Ω —Ç—Ä–∏–≥–≥–µ—Ä–∏—Ç—å fallback
        assert should_fallback is False

    @pytest.mark.asyncio
    async def test_should_not_fallback_on_connection_error(self, test_config: Config) -> None:
        """
        –¢–µ—Å—Ç: Connection –æ—à–∏–±–∫–∏ –ù–ï —Ç—Ä–∏–≥–≥–µ—Ä—è—Ç fallback.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        """
        # Arrange: –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ —Å fallback –º–æ–¥–µ–ª—å—é
        test_config.openrouter_fallback_model = "meta-llama/llama-3.1-8b-instruct:free"
        llm_client = LLMClient(test_config)

        # –°–æ–∑–¥–∞—ë–º APIConnectionError
        error = APIConnectionError(request=AsyncMock())

        # Act: –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–ª–∂–µ–Ω –ª–∏ —Å—Ä–∞–±–æ—Ç–∞—Ç—å fallback
        should_fallback = llm_client._should_try_fallback(error)

        # Assert: Connection error –ù–ï –¥–æ–ª–∂–µ–Ω —Ç—Ä–∏–≥–≥–µ—Ä–∏—Ç—å fallback
        assert should_fallback is False

    @pytest.mark.asyncio
    async def test_fallback_on_primary_model_failure(
        self, test_config: Config, sample_messages: list[dict[str, str]]
    ) -> None:
        """
        –¢–µ—Å—Ç: —É—Å–ø–µ—à–Ω—ã–π fallback –ø—Ä–∏ –ø—Ä–æ–≤–∞–ª–µ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            sample_messages: –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        # Arrange: –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º fallback –º–æ–¥–µ–ª—å
        test_config.openrouter_fallback_model = "meta-llama/llama-3.1-8b-instruct:free"
        llm_client = LLMClient(test_config)
        user_id = 12345

        # Mock: –æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–æ–≤–∞–ª–∏–ª–∞—Å—å —Å RateLimitError, fallback —É—Å–ø–µ—à–µ–Ω
        mock_client = AsyncMock()
        mock_response = MagicMock()
        mock_response.request = MagicMock()

        # Fallback –æ—Ç–≤–µ—Ç
        mock_choice = AsyncMock()
        mock_choice.message.content = "–û—Ç–≤–µ—Ç –æ—Ç fallback –º–æ–¥–µ–ª–∏"
        mock_completion = AsyncMock()
        mock_completion.choices = [mock_choice]
        mock_completion.usage.prompt_tokens = 10
        mock_completion.usage.completion_tokens = 5
        mock_completion.usage.total_tokens = 15

        # –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å: 3 –ø—Ä–æ–≤–∞–ª–∞, –ø–æ—Ç–æ–º fallback —É—Å–ø–µ—Ö
        mock_client.chat.completions.create.side_effect = [
            RateLimitError("Rate limit", response=mock_response, body=None),
            RateLimitError("Rate limit", response=mock_response, body=None),
            RateLimitError("Rate limit", response=mock_response, body=None),
            mock_completion,  # Fallback —É—Å–ø–µ—Ö
        ]

        llm_client.client = mock_client

        # Act: –≤—ã–∑—ã–≤–∞–µ–º generate_response
        response = await llm_client.generate_response(sample_messages, user_id)

        # Assert: –ø–æ–ª—É—á–∏–ª–∏ –æ—Ç–≤–µ—Ç –æ—Ç fallback –º–æ–¥–µ–ª–∏
        assert response == "–û—Ç–≤–µ—Ç –æ—Ç fallback –º–æ–¥–µ–ª–∏"
        # 3 –ø–æ–ø—ã—Ç–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ + 1 —É—Å–ø–µ—à–Ω—ã–π fallback
        assert mock_client.chat.completions.create.call_count == 4

    @pytest.mark.asyncio
    async def test_no_fallback_without_config(
        self, test_config: Config, sample_messages: list[dict[str, str]]
    ) -> None:
        """
        –¢–µ—Å—Ç: fallback –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ—Å–ª–∏ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            sample_messages: –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        # Arrange: –ù–ï –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º fallback –º–æ–¥–µ–ª—å
        test_config.openrouter_fallback_model = None
        llm_client = LLMClient(test_config)
        user_id = 12345

        # Mock: –æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–æ–≤–∞–ª–∏–ª–∞—Å—å
        mock_client = AsyncMock()
        mock_response = MagicMock()
        mock_response.request = MagicMock()

        mock_client.chat.completions.create.side_effect = RateLimitError(
            "Rate limit", response=mock_response, body=None
        )

        llm_client.client = mock_client

        # Act & Assert: –¥–æ–ª–∂–Ω–æ –≤—ã–±—Ä–æ—Å–∏—Ç—å—Å—è –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
        with pytest.raises(LLMAPIError) as exc_info:
            await llm_client.generate_response(sample_messages, user_id)

        assert "Rate limit exceeded" in str(exc_info.value)
        # –¢–æ–ª—å–∫–æ 3 –ø–æ–ø—ã—Ç–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏, –Ω–µ—Ç fallback
        assert mock_client.chat.completions.create.call_count == 3


class TestLLMClientEdgeCases:
    """–¢–µ—Å—Ç—ã edge cases –¥–ª—è LLMClient."""

    @pytest.mark.asyncio
    async def test_unicode_and_emoji_in_messages(
        self,
        test_config: Config,
        mock_openai_client: AsyncMock,
    ) -> None:
        """
        –¢–µ—Å—Ç: –æ–±—Ä–∞–±–æ—Ç–∫–∞ Unicode —Å–∏–º–≤–æ–ª–æ–≤ –∏ —ç–º–æ–¥–∑–∏.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            mock_openai_client: Mock –∫–ª–∏–µ–Ω—Ç–∞ OpenAI
        """
        llm_client = LLMClient(test_config)
        llm_client.client = mock_openai_client
        user_id = 12345

        # –°–æ–æ–±—â–µ–Ω–∏—è —Å Unicode –∏ —ç–º–æ–¥–∑–∏
        messages = [
            {"role": "system", "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ ü§ñ"},
            {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! üëã –ö–∞–∫ –¥–µ–ª–∞? ‰Ω†Â•Ω"},
            {"role": "assistant", "content": "–û—Ç–ª–∏—á–Ω–æ! üòä –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å? üåü"},
        ]

        response = await llm_client.generate_response(messages, user_id)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–∞–ø—Ä–æ—Å –ø—Ä–æ—à–µ–ª
        assert response == "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM."
        mock_openai_client.chat.completions.create.assert_called_once()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç–º–æ–¥–∑–∏ –∏ unicode –ø–µ—Ä–µ–¥–∞–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
        call_kwargs = mock_openai_client.chat.completions.create.call_args.kwargs
        assert "ü§ñ" in str(call_kwargs["messages"])
        assert "üëã" in str(call_kwargs["messages"])
        assert "‰Ω†Â•Ω" in str(call_kwargs["messages"])

    @pytest.mark.asyncio
    async def test_very_long_message(
        self,
        test_config: Config,
        mock_openai_client: AsyncMock,
    ) -> None:
        """
        –¢–µ—Å—Ç: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (>10k —Å–∏–º–≤–æ–ª–æ–≤).

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            mock_openai_client: Mock –∫–ª–∏–µ–Ω—Ç–∞ OpenAI
        """
        llm_client = LLMClient(test_config)
        llm_client.client = mock_openai_client
        user_id = 12345

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (>10k —Å–∏–º–≤–æ–ª–æ–≤)
        long_content = "–ê" * 15000  # 15k —Å–∏–º–≤–æ–ª–æ–≤

        messages = [
            {"role": "system", "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫"},
            {"role": "user", "content": long_content},
        ]

        response = await llm_client.generate_response(messages, user_id)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–∞–ø—Ä–æ—Å –ø—Ä–æ—à–µ–ª
        assert response == "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM."
        mock_openai_client.chat.completions.create.assert_called_once()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–µ—Ä–µ–¥–∞–Ω–æ
        call_kwargs = mock_openai_client.chat.completions.create.call_args.kwargs
        assert len(call_kwargs["messages"][1]["content"]) == 15000

    @pytest.mark.asyncio
    async def test_empty_string_in_messages(
        self,
        test_config: Config,
        mock_openai_client: AsyncMock,
    ) -> None:
        """
        –¢–µ—Å—Ç: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            mock_openai_client: Mock –∫–ª–∏–µ–Ω—Ç–∞ OpenAI
        """
        llm_client = LLMClient(test_config)
        llm_client.client = mock_openai_client
        user_id = 12345

        # –°–æ–æ–±—â–µ–Ω–∏—è —Å –ø—É—Å—Ç—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏
        messages = [
            {"role": "system", "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫"},
            {"role": "user", "content": ""},  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
            {"role": "assistant", "content": "OK"},
            {"role": "user", "content": "   "},  # –¢–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã
        ]

        response = await llm_client.generate_response(messages, user_id)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–∞–ø—Ä–æ—Å –ø—Ä–æ—à–µ–ª
        assert response == "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM."
        mock_openai_client.chat.completions.create.assert_called_once()

    @pytest.mark.asyncio
    async def test_special_characters_in_messages(
        self,
        test_config: Config,
        mock_openai_client: AsyncMock,
    ) -> None:
        """
        –¢–µ—Å—Ç: –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö.

        Args:
            test_config: –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            mock_openai_client: Mock –∫–ª–∏–µ–Ω—Ç–∞ OpenAI
        """
        llm_client = LLMClient(test_config)
        llm_client.client = mock_openai_client
        user_id = 12345

        # –°–æ–æ–±—â–µ–Ω–∏—è —Å–æ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–∞–º–∏
        messages = [
            {"role": "system", "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫"},
            {
                "role": "user",
                "content": "–ü—Ä–∏–≤–µ—Ç\\n\\t<script>alert('test')</script>\\r\\n\"\\'",
            },
        ]

        response = await llm_client.generate_response(messages, user_id)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–∞–ø—Ä–æ—Å –ø—Ä–æ—à–µ–ª –±–µ–∑ –æ—à–∏–±–æ–∫
        assert response == "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM."
        mock_openai_client.chat.completions.create.assert_called_once()
