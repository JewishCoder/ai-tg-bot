"""–¢–µ—Å—Ç—ã –¥–ª—è —É—Ç–∏–ª–∏—Ç—ã sanitization –ª–æ–≥–æ–≤."""

from src.utils import sanitize_content, sanitize_token


def test_sanitize_content_empty() -> None:
    """–¢–µ—Å—Ç: –ø—É—Å—Ç–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ."""
    result = sanitize_content("", show_content=False)
    assert result == "[empty]"


def test_sanitize_content_hidden() -> None:
    """–¢–µ—Å—Ç: —Å–∫—Ä—ã—Ç–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ (production —Ä–µ–∂–∏–º)."""
    text = "–°–µ–∫—Ä–µ—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"
    result = sanitize_content(text, show_content=False)

    assert "[" in result
    assert "chars]" in result
    assert "–°–µ–∫—Ä–µ—Ç–Ω–æ–µ" not in result
    assert len(text) == len("–°–µ–∫—Ä–µ—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")


def test_sanitize_content_show_short() -> None:
    """–¢–µ—Å—Ç: –ø–æ–∫–∞–∑ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ (dev —Ä–µ–∂–∏–º)."""
    text = "–ö–æ—Ä–æ—Ç–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"
    result = sanitize_content(text, show_content=True, max_length=50)

    assert result == text


def test_sanitize_content_show_long() -> None:
    """–¢–µ—Å—Ç: –ø–æ–∫–∞–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å –æ–±—Ä–µ–∑–∫–æ–π (dev —Ä–µ–∂–∏–º)."""
    text = "–ê" * 100
    result = sanitize_content(text, show_content=True, max_length=50)

    assert result.startswith("–ê" * 50)
    assert "..." in result
    assert "100 chars total" in result


def test_sanitize_content_unicode() -> None:
    """–¢–µ—Å—Ç: –æ–±—Ä–∞–±–æ—Ç–∫–∞ Unicode —Å–∏–º–≤–æ–ª–æ–≤."""
    text = "–ü—Ä–∏–≤–µ—Ç! üëã ‰Ω†Â•Ω"
    result_hidden = sanitize_content(text, show_content=False)
    result_shown = sanitize_content(text, show_content=True)

    # –í —Å–∫—Ä—ã—Ç–æ–º —Ä–µ–∂–∏–º–µ —Ç–æ–ª—å–∫–æ –¥–ª–∏–Ω–∞
    assert "chars]" in result_hidden

    # –í –æ—Ç–∫—Ä—ã—Ç–æ–º —Ä–µ–∂–∏–º–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
    assert "–ü—Ä–∏–≤–µ—Ç" in result_shown
    assert "üëã" in result_shown


def test_sanitize_token_standard() -> None:
    """–¢–µ—Å—Ç: sanitize API —Ç–æ–∫–µ–Ω–∞."""
    token = "sk-test1234567890abcdefghij"
    result = sanitize_token(token, show_length=4)

    assert result == "sk-t...ghij"
    assert len(result) < len(token)


def test_sanitize_token_short() -> None:
    """–¢–µ—Å—Ç: –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–æ–∫–µ–Ω."""
    token = "abc"
    result = sanitize_token(token, show_length=4)

    assert result == "***"


def test_sanitize_token_empty() -> None:
    """–¢–µ—Å—Ç: –ø—É—Å—Ç–æ–π —Ç–æ–∫–µ–Ω."""
    result = sanitize_token("", show_length=4)

    assert result == "***"


def test_sanitize_content_length_reporting() -> None:
    """–¢–µ—Å—Ç: –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª–∏–Ω—ã."""
    text = "Test message with some content"
    result = sanitize_content(text, show_content=False)

    # –î–æ–ª–∂–µ–Ω –ø–æ–∫–∞–∑–∞—Ç—å –¥–ª–∏–Ω—É
    assert f"[{len(text)} chars]" == result
